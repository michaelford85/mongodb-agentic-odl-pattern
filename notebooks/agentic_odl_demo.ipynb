{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef7KwuM9N7Vd"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/michaelford85/atlas-rag-pipeline/blob/main/atlas_rag_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlas RAG Pipeline â€” Retrieval-Augmented Generation with MongoDB & Ollama \n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** workflow built on:\n",
    "\n",
    "- **MongoDB Atlas Vector Search** for semantic retrieval  \n",
    "- **VoyageAI embeddings** for context-rich text representations  \n",
    "- **Ollama (Mistral model)** for local, private LLM inference  \n",
    "\n",
    "The goal: retrieve semantically relevant text from MongoDB Atlas and generate natural language answers using a locally hosted large language model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-_7c8Ed2hoV"
   },
   "source": [
    "## Clone the Project Repository\n",
    "\n",
    "Before running the RAG pipeline, we install dependencies and pull the latest version of the code from GitHub.  \n",
    "This cell:\n",
    "\n",
    "1. Updates the system package list  \n",
    "2. Installs `git` (if not already available)  \n",
    "3. Removes any previous copy of the project  \n",
    "4. Clones the **atlas-rag-pipeline** repository from GitHub  \n",
    "\n",
    "This ensures that the notebook always uses the most recent version of the pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "762fhofuNKmt"
   },
   "outputs": [],
   "source": [
    "!apt-get update -qq > /dev/null\n",
    "!apt-get install --yes git > /dev/null\n",
    "!rm -rf atlas-rag-pipeline\n",
    "!git clone https://github.com/michaelford85/atlas-rag-pipeline.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxZPcGfG2sFc"
   },
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We begin by importing Python libraries used throughout the pipeline:  \n",
    "\n",
    "- **pymongo** â€“ to connect to MongoDB Atlas  \n",
    "- **voyageai** â€“ for embedding generation  \n",
    "- **requests** â€“ to communicate with the local Ollama API  \n",
    "- **dotenv_vault** â€“ for securely loading environment variables  \n",
    "- **certifi** â€“ for verified SSL certificates in Atlas connections  \n",
    "- **IPython.display** â€“ for interactive display elements  \n",
    "\n",
    "These imports set up the environment for retrieval, generation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CHJ1UgOhOWrP"
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir -r atlas-rag-pipeline/requirements.txt\n",
    "!pip install --upgrade \"docutils>=0.20,<0.22\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQPV2xv63uP-"
   },
   "source": [
    "## Access Secure Environment Variables from Google Drive\n",
    "\n",
    "If youâ€™re running this notebook in **Google Colab**, you have two options for loading your environment variables:\n",
    "If youâ€™re running this notebook in **Google Colab**, you can securely load your private `.env.vault` key from Google Drive.  \n",
    "\n",
    "### Option 1: Secure (Recommended): Load from Google Drive\n",
    "\n",
    "This approach keeps your credentials out of the notebook and automatically decrypts values from your `.env.vault` file.\n",
    "#### Steps:\n",
    "1. Mount your Google Drive so the notebook can access stored files.\n",
    "2. Read your Dotenv Vault key and ID (e.g., `atlas_rag_pipeline_voyageai_dotenv_key.txt` and `atlas_rag_pipeline_voyageai_dotenv_vault_id.txt`) from your Drive folder.\n",
    "3. Set the environment variables so the notebook can decrypt your .env.vault.\n",
    "\n",
    "This approach keeps sensitive credentials out of the notebook while allowing you to authenticate seamlessly during Colab sessions.\n",
    "\n",
    " **NOTE**: Ignore any funky formatting in the commands to retrieve the `.env.vault` file, it is a result of mixing `!` shell commands and Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WEaSArQO7f9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# Load Dotenv Vault credentials from Google Drive\n",
    "os.environ[\"DOTENV_KEY\"] = open(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/secrets/atlas_rag_pipeline_voyageai_dotenv_key.txt'\n",
    ").read().strip()\n",
    "\n",
    "os.environ[\"DOTENV_VAULT_ID\"] = open(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/secrets/atlas_rag_pipeline_voyageai_dotenv_vault_id.txt'\n",
    ").read().strip()\n",
    "\n",
    "# --- Sync Dotenv Vault ---\n",
    "!npx dotenv-vault@latest new $DOTENV_VAULT_ID\n",
    "!DOTENV_KEY=$DOTENV_KEY \\\n",
    "  npx dotenv-vault@latest pull development atlas-rag-pipeline/.env.vault\n",
    "!ls -la atlas-rag-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opNREd1n21Jz"
   },
   "source": [
    "### Option 2: Simple â€” Use Dummy or Local Values\n",
    "\n",
    "If youâ€™re running locally (or donâ€™t want to mount Google Drive), you can set placeholder values instead.\n",
    "These wonâ€™t decrypt a `.env.vault`, but they let you execute the notebook without secrets.\n",
    "\n",
    "## NOTE \n",
    "- **Never commit real keys or vault IDs to GitHub**.\n",
    "- Dummy values are suitable for demonstration or dry-run mode.\n",
    "- For production use, always load credentials from an encrypted vault or a local .env.vault file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bANTeRHEUypm"
   },
   "outputs": [],
   "source": [
    "## Replace all values below with your actual credentials - DO NOT COMMIT REAL KEYS TO GITHUB!\n",
    "\n",
    "## API key for VoyageAI embedding service.\n",
    "os.environ[\"VOYAGE_API_KEY\"] = \"v2_dummykey_abc123xyz789\"\n",
    "\n",
    "## MongoDB Atlas connection details.\n",
    "os.environ[\"MONGODB_URI\"] = \"mongodb+srv://demo_user:demo_password@cluster0.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "## MongoDB Atlas API public key and private key\n",
    "os.environ[\"ATLAS_PUBLIC_KEY\"] = \"abcd1234efgh5678ijkl\"\n",
    "os.environ[\"ATLAS_PRIVATE_KEY\"] = \"mnop5678qrst9012uvwx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU0es93P36QX"
   },
   "source": [
    "# ðŸ§  Generate and Manage Vector Embeddings in MongoDB\n",
    "\n",
    "Before running retrieval or semantic search queries, we need to make sure our **MongoDB Atlas collection** contains up-to-date vector embeddings and a properly configured **Atlas Vector Search index**.\n",
    "\n",
    "This section runs a coordinated set of helper scripts that prepare the collection for vector-based querying:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ `update_voyage_ai_embeddings.py`\n",
    "Uses the [Voyage AI](https://www.voyageai.com) API to generate (or refresh) embeddings for each documentâ€™s text fields defined in `EMBEDDING_PATHS`.\n",
    "\n",
    "- Each field listed in `EMBEDDING_PATHS` is embedded and stored under a corresponding field in `EMBEDDING_NAMES`.  \n",
    "  For example:\n",
    "  - `\"fullplot\"` â†’ `\"fullplot_embedding\"`\n",
    "  - `\"plot\"` â†’ `\"plot_embedding\"`\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ `manage_vector_index.py`\n",
    "Creates or ensures the existence of an **Atlas Vector Search index** referencing all embedding fields.\n",
    "\n",
    "- Enables `$vectorSearch` queries over one or more embedding vectors (e.g., `\"fullplot_embedding\"` and `\"plot_embedding\"`).  \n",
    "- Waits until the index is fully built before exiting, guaranteeing itâ€™s ready for queries.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ *(Optional)* `remove_embeddings.py`\n",
    "Removes existing embedding fields if you need to regenerate them or rebuild the index from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "Together, these scripts automate the preparation process for **semantic retrieval** and **Retrieval-Augmented Generation (RAG)** workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dE_hdWEEVy6_"
   },
   "outputs": [],
   "source": [
    "## Replace all values below with your actual database values\n",
    "\n",
    "import os\n",
    "os.environ[\"DB_NAME\"] = \"sample_mflix\"\n",
    "os.environ[\"COLL_NAME\"] = \"movies\"\n",
    "os.environ[\"BATCH_SIZE\"] = \"100\"\n",
    "os.environ[\"MODEL_NAME\"] = \"voyage-3-large\"\n",
    "os.environ[\"NUM_DIMENSIONS\"] = \"1024\"\n",
    "os.environ[\"ATLAS_GROUP_ID\"] = \"000000000000000000000000\"\n",
    "os.environ[\"ATLAS_CLUSTER\"] = \"demo-cluster\"\n",
    "\n",
    "# An array of embedding paths and names for making a vector index, separated by commas\n",
    "os.environ[\"EMBEDDING_PATHS\"] = \"fullplot,plot\"\n",
    "os.environ[\"EMBEDDING_NAMES\"] = \"fullplot_embedding,plot_embedding\"\n",
    "\n",
    "# The Vector Index based on the embedding names specified above\n",
    "os.environ[\"INDEX_NAME\"] = \"fullplot_vector_index\"\n",
    "\n",
    "# Create or update embeddings and vector search index\n",
    "%run atlas-rag-pipeline/update_voyage_ai_embeddings.py\n",
    "%run atlas-rag-pipeline/manage_vector_index.py\n",
    "\n",
    "#Option to remove all embedding fields from the specified collection\n",
    "# %run atlas-rag-pipeline/remove_embeddings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is9XzJeP4C9I"
   },
   "source": [
    "## Set Up Ollama on Ubuntu with Ansible\n",
    "\n",
    "This step uses **Ansible** to automatically install and configure **Ollama** on an Ubuntu environment.  \n",
    "Running this playbook ensures that the local system (or Google Colab VM) has the Ollama service and dependencies properly set up.\n",
    "\n",
    "Specifically, this command:\n",
    "- Installs **Ollama** and its required packages  \n",
    "- Configures the **systemd service** so Ollama runs continuously  \n",
    "- Ensures the **Mistral model** can be pulled and served via API at `http://localhost:11434`\n",
    "\n",
    "This automation makes your environment consistent and repeatable across different machines or Google Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_FtdRdSlQwa"
   },
   "outputs": [],
   "source": [
    "!ansible-playbook atlas-rag-pipeline/setup_ollama_ubuntu.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbE12wwN2Yaa"
   },
   "source": [
    "## Enable Line Wrapping for Readable Output\n",
    "\n",
    "By default, Jupyter and Google Colab often display long text outputs on a single line, requiring horizontal scrolling.  \n",
    "This helper function injects a small CSS rule into the notebook that enables **automatic text wrapping** inside output cells.\n",
    "\n",
    "Every time a new cell runs, the `pre_run_cell` event re-applies the style so wrapping stays consistent throughout the notebook â€” perfect for displaying long model responses or logs neatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHWfiabBsDOz"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S2a5EEW4T5r"
   },
   "source": [
    "## Test the Local Mistral Model via Ollama API\n",
    "\n",
    "Before integrating Ollama into the full RAG pipeline, itâ€™s important to verify that the local **Mistral** model is running correctly and capable of generating responses.\n",
    "\n",
    "This command sends a test prompt directly to the **Ollama REST API** at `http://localhost:11434/api/generate`.  \n",
    "It asks a simple, knowledge-based question about preventing frozen pipes and expects a concise, paragraph-style answer.\n",
    "\n",
    "The command:\n",
    "- Uses `curl` to send a JSON payload with the model name (`mistral`) and prompt text  \n",
    "- Disables streaming for easier readability  \n",
    "- Pipes the output through `jq` to extract and display only the `.response` field  \n",
    "\n",
    "If the setup is correct, this cell should output a coherent answer â€” confirming that the local generative AI model is online and functioning properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwwnNOHI9TkF"
   },
   "outputs": [],
   "source": [
    "!curl -s http://localhost:11434/api/generate \\\n",
    "  -d '{{\"model\": \"mistral\", \"prompt\": \"What should homeowners do in order to keep their pipes from freezing in the winter? Keep your explanation as a paragraph of 10 sentences or less.\", \"stream\": false}}' \\\n",
    "  | jq -r '.response'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tQRSZaz4iwn"
   },
   "source": [
    "## Run the Full RAG Pipeline with a Custom Prompt\n",
    "\n",
    "Now that both the **retrieval** and **generation** components are configured, we can run the complete **Retrieval-Augmented Generation (RAG)** pipeline end-to-end.  \n",
    "\n",
    "This command executes `rag_with_input.py`, which:\n",
    "1. Takes a natural-language query as input (in this case: *â€œWhich movies feature artificial intelligence or sentient robots?â€*)  \n",
    "2. Generates embeddings using **VoyageAI**  \n",
    "3. Retrieves semantically similar movie plots from **MongoDB Atlas**  \n",
    "4. Passes the retrieved context to the **Mistral** model via **Ollama**  \n",
    "5. Returns a generated answer that blends factual retrieval with fluent reasoning  \n",
    "\n",
    "During discussions or demos, you can modify the quoted text to explore different topics â€” testing how the system responds to various prompts in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdAY2OJHayp0"
   },
   "outputs": [],
   "source": [
    "%run atlas-rag-pipeline/rag_with_input.py \"Which movies feature artificial intelligence or sentient robots?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Gradio\n",
    "\n",
    "Before launching the RAG Query interface, install **Gradio**, a lightweight web UI framework for running interactive machine learning demos directly in Google Colab or your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJu99jfEiY89"
   },
   "outputs": [],
   "source": [
    "!pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the MongoDB RAG Query Assistant\n",
    "\n",
    "This section launches a simple Gradio interface that connects to your MongoDB Vector Search index and allows you to ask natural-language questions.\n",
    "\n",
    "The interface:\n",
    "- Calls your retrieve_relevant_docs() and generate_answer() functions.\n",
    "- Returns the generated answer in a scrollable textbox.\n",
    "- Provides a â€œCopy Outputâ€ button for convenience.\n",
    "\n",
    "How it works:\n",
    "- You enter a question in the input box.\n",
    "- The notebook retrieves semantically relevant documents from MongoDB Atlas.\n",
    "- It uses your local model (e.g., Ollama with Mistral) to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygG_T_dwiOoQ"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import sys\n",
    "sys.path.append('/content/atlas-rag-pipeline')\n",
    "from rag_with_input import retrieve_relevant_docs, generate_answer\n",
    "\n",
    "def rag_query(question):\n",
    "    docs = retrieve_relevant_docs(question, limit=3)\n",
    "    if not docs:\n",
    "        return \"âš ï¸ No relevant documents found.\"\n",
    "    answer = generate_answer(question, docs)\n",
    "    return answer\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=rag_query,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"Ask a Question\",\n",
    "        placeholder=\"Type your query here...\",\n",
    "        lines=2,       # input box height\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        label=\"Output\",\n",
    "        lines=15,      # ðŸ‘ˆ make output area much taller\n",
    "        show_copy_button=True\n",
    "    ),\n",
    "    title=\"MongoDB RAG Query Assistant\",\n",
    "    description=\"Ask questions against your MongoDB vector index.\",\n",
    ")\n",
    "\n",
    "app = demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gracefully Shut Down the Gradio App\n",
    "\n",
    "When youâ€™re done using the interface (or want to restart it with updated code),  \n",
    "you can safely close all active Gradio instances by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWYWoM5njvQ9"
   },
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5kaKZGd5F0a"
   },
   "source": [
    "## Restart the Ollama Service\n",
    "\n",
    "Occasionally, especially during long Colab sessions or multiple test runs, the **Ollama service** may hold onto old connections or become unresponsive.  \n",
    "This cell cleanly restarts the local Ollama server to ensure a fresh, stable session before generating new responses.\n",
    "\n",
    "Hereâ€™s what each command does:\n",
    "1. `pkill ollama` â€” Stops any existing Ollama processes (ignores errors if none are running).  \n",
    "2. `nohup ollama serve > /tmp/ollama.log 2>&1 &` â€” Restarts Ollama in the background and logs output silently.  \n",
    "3. `sleep 10` â€” Waits a few seconds to allow the server to fully initialize before accepting API requests.\n",
    "\n",
    "Running this cell helps maintain reliable performance and prevents connection errors when calling the Mistral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "G38L92tgtbFj"
   },
   "outputs": [],
   "source": [
    "!pkill ollama || true\n",
    "!nohup ollama serve > /tmp/ollama.log 2>&1 &\n",
    "!sleep 10"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNxpfj9Oze9wWOFXYZ2KB3G",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
